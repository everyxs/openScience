"Tag","PaperId","AuthorIdsOrder","AuthorNamesOrder","authorCount","unknownCount","femaleCount","femaleProp","X1st","X2nd","X3rd","X4th","X5th","last","FoSNames","Year","DocType","Journal","Publisher","Doi","Title","EstimatedCitation","IndexedAbstract"
"Reproducibility",2284191284,"2087863783","Richard R J Cousley",1,0,0,0,0,NA,NA,NA,NA,0,"reproducibility; medicine; orthodontics",2015,"Journal","Journal of Orthodontics","Taylor & Francis","10.1080/14653125.2015.1122915","Accuracy and Reproducibility Of Linear Measurements Of Resin, Plaster, Digital and Printed Study-Models",0,NA
"OpenScience",2024986130,"705885879; 2114464965; 2267980870","Amye Kenall; Simon Harold; Christopher Foote",3,0,1,0.333333333333333,1,0,0,NA,NA,0,"entomology; open science; open data; biology; data sharing; evolutionary biology; ecology and evolutionary biology",2014,"Journal","BMC Evolutionary Biology","BioMed Central Ltd","10.1186/1471-2148-14-66","An open future for ecological and evolutionary data",2,"As part of BioMed Central’s open science mission, we are pleased to announce that two of our journals have integrated with the open data repository Dryad. Authors submitting their research to either BMC Ecology or BMC Evolutionary Biology will now have the opportunity to deposit their data directly into the Dryad archive and will receive a permanent, citable link to their dataset. Although this does not affect any of our current data deposition policies at these journals, we hope to encourage a more widespread adoption of open data sharing in the fields of ecology and evolutionary biology by facilitating this process for our authors. We also take this opportunity to discuss some of the wider issues that may concern researchers when making their data openly available. Although we offer a number of positive examples from different fields of biology, we also recognise that reticence to data sharing still exists, and that change must be driven from within research communities in order to create future science that is fit for purpose in the digital age."
"Reproducibility",1990981932,"705885879; 1976589559; 2412316283; 2132292370; 2497373859; 2160563883; 2505586523","Amye Kenall; Scott Edmunds; Laurie Goodman; Liz Bal; Louisa Flintoft; Daniel R Shanahan; Tim Shipley",7,0,4,0.571428571428571,1,0,1,1,1,0,"checklist; bioinformatics; reproducibility; replicate; computer science",2015,"Journal","GigaScience","BioMed Central Ltd","10.1186/s13742-015-0071-8","Better reporting for better research: a checklist for reproducibility",4,"How easy is it to reproduce or replicate the findings of a published paper? In 2013 one researcher, Phil Bourne, asked just this. How easy would it be to reproduce the results of a computational biology paper? [1]. The answer: 280 hours. Such a number is surprising, given the theoretical reproducibility of computational research and given Bourne was attempting to reproduce work done in his own lab. Now at the National Institutes of Health (NIH) as Associate Director of Data Sciences, Bourne is concerned with the reproducibility of all NIH funded work, not just his own—and the problem is large. In addition to work in computational biology (which theoretically should be more easily reproducible than “wet lab” work), hallmark papers in cancer through to psychology have been flagged as largely unreproducible [2, 3]. Closer to home, GigaScience has carried out similar work to quantify reproducibility in their content. Despite being scrutinized and tested by seven referees, it still took about half a man-month worth of resources to reproduce the results reported in just one of the tables [4]. “Reproducibility” is now increasingly on the radar of funders and is making its rounds in the wider media as well, with concerns of reproducibility making headlines at The Economist [5] and New York Times [6], amongst other outlets."
"Reproducibility",2100555847,"705885879; 1976589559; 2412316283; 2132292370; 2497373859; 2160563883; 2505586523","Amye Kenall; Scott Edmunds; Laurie Goodman; Liz Bal; Louisa Flintoft; Daniel R Shanahan; Tim Shipley",7,0,4,0.571428571428571,1,0,1,1,1,0,"checklist; reproducibility; replicate; biology; research design; bioinformatics; genetics",2015,"Journal","Genome Biology","BioMed Central","10.1186/s13059-015-0710-5","Better reporting for better research: a checklist for reproducibility",4,"How easy is it to reproduce or replicate the findings of a published paper? In 2013 one researcher, Phil Bourne, asked just this. How easy would it be to reproduce the results of a computational biology paper? [1]. The answer: 280 hours. Such a number is surprising, given the theoretical reproducibility of computational research and given Bourne was attempting to reproduce work done in his own lab. Now at the National Institutes of Health (NIH) as Associate Director of Data Sciences, Bourne is concerned with the reproducibility of all NIH funded work, not just his own—and the problem is large. In addition to work in computational biology (which theoretically should be more easily reproducible than “wet lab” work), hallmark papers in cancer through to psychology have been flagged as largely unreproducible [2, 3]. Closer to home, GigaScience has carried out similar work to quantify reproducibility in their content. Despite being scrutinized and tested by seven referees, it still took about half a man-month worth of resources to reproduce the results reported in just one of the tables [4]. “Reproducibility” is now increasingly on the radar of funders and is making its rounds in the wider media as well, with concerns of reproducibility making headlines at The Economist [5] and New York Times [6], amongst other outlets."
"Reproducibility",2621103699,"216036206","David Trafimow",1,0,0,0,0,NA,NA,NA,NA,0,"social psychology; psychology; reproducibility",2017,"Journal","Frontiers in Psychology","Frontiers","10.3389/fpsyg.2017.00918","Commentary: Reproducibility in Psychological Science: When Do Psychological Phenomena Exist?",0,NA
"Reproducibility",2034972036,"2667069942","Harvey J. Motulsky",1,0,0,0,0,NA,NA,NA,NA,0,"computer science; standard error; text mining; reproducibility; data science; statistical hypothesis testing; data mining",2015,"Journal","Pharmacology Research & Perspectives","Pharmacol Res Perspect","10.1002/prp2.93","Common misconceptions about data analysis and statistics",8,"Ideally, any experienced investigator with the right tools should be able to reproduce a finding published in a peer-reviewed biomedical science journal. In fact, the reproducibility of a large percentage of published findings has been questioned. Undoubtedly, there are many reasons for this, but one reason may be that investigators fool themselves due to a poor understanding of statistical concepts. In particular, investigators often make these mistakes: (1) P-Hacking. This is when you reanalyze a data set in many different ways, or perhaps reanalyze with additional replicates, until you get the result you want. (2) Overemphasis on P values rather than on the actual size of the observed effect. (3) Overuse of statistical hypothesis testing, and being seduced by the word “significant”. (4) Overreliance on standard errors, which are often misunderstood."
"Reproducibility",1971489609,"2633604498","Harvey Motulsky",1,0,0,0,0,NA,NA,NA,NA,0,"statistics; standard error; reproducibility; medicine; statistical hypothesis testing",2014,"Journal","Naunyn-schmiedebergs Archives of Pharmacology","Naunyn Schmiedebergs Arch Pharmacol","10.1007/s00210-014-1037-6","Common misconceptions about data analysis and statistics",14,"Ideally, any experienced investigator with the right tools should be able to reproduce a finding published in a peer-reviewed biomedical science journal. In fact, the reproducibility of a large percentage of published findings has been questioned. Undoubtedly, there are many reasons for this, but one reason maybe that investigators fool themselves due to a poor understanding of statistical concepts. In particular, investigators often make these mistakes: 1. P-Hacking. This is when you reanalyze a data set in many different ways, or perhaps reanalyze with additional replicates, until you get the result you want. 2. Overemphasis on P values rather than on the actual size of the observed effect. 3. Overuse of statistical hypothesis testing, and being seduced by the word “significant”. 4. Overreliance on standard errors, which are often misunderstood."
"Reproducibility",2058618110,"2768209340","Emilie Marcus",1,0,1,1,1,NA,NA,NA,NA,1,"biology; genetics; reproducibility; credibility; knowledge management; transparency",2014,"Journal","Molecular Cell","Elsevier","10.1016/j.molcel.2014.11.012","Credibility and reproducibility.",1,"Credibility is everything for science, and it is built over time in both obvious and subtle ways. It is how we interact with colleagues and collaborators. It is how generously and openly we share reagents and how we mentor students and postdocs. It is how we review each other’s papers, and it is how we credit others' work. It is the way we educate and inform the public that funds us. It is the way we document and store our data. And it is the rigor, transparency, and attention we invest in designing, conducting, and reporting experiments."
"Reproducibility",2318955727,"2653667958; 2646808135; 2651657820; 2687886206; 2676442317; 2485740020; 2651964484","Sm Lobmaier; M Cruz-Lemini; B Valenzuela-Alcaraz; Ju Ortiz; Jm Martinez; E. Gratacós; F Crispi",7,5,1,0.5,NA,NA,NA,1,0,NA,"repeatability; medicine; pediatrics; reproducibility; radiology; obstetrics",2014,"Journal","Geburtshilfe Und Frauenheilkunde",NA,"10.1055/s-0034-1388045","Influence of equipment and settings on myocardial performance index repeatability and definition of settings to achieve optimal reproducibility",0,NA
"Reproducibility",2317589598,"2650686529; 2051862677; 2158812809; 2050622594; 2513997452","M. Asrar Ul Haq; Vivek Mutha; Simon Stewart; M. Carrington; Chun Wai Wong",5,2,0,0,NA,0,0,NA,0,0,"cardiology; reproducibility; internal medicine; diastolic function; medicine",2013,"Journal","European Heart Journal",NA,"10.1093/eurheartj/eht308.P1117","Interstudy reproducibility of echocardiographic parameters in the serial assessment of left ventricular diastolic function",0,NA
"OpenScience",2751591353,"2752293879","Doreen Siegfried",1,0,1,1,1,NA,NA,NA,NA,1,"art; open science; library science",2017,"Journal",NA,NA,"10.15358/0340-1650-2017-7-8-54","Open Science: Das neue Gold von Wissenschaft sind Forschungsdaten",0,NA
"Reproducibility",2333058176,"2679623471; 2591534458; 2590667453; 2590743729; 2591198797; 2589869046; 2591106474; 2779245310","Jun Okamoto; Yu Kumasaka; Kazuya Kawamura; Tomoyuki Matsumoto; Seiji Kubo; Hirotsugu Muratsu; Masahiro Kurosaka; Masakatsu G. Fujie",8,5,0,0,0,0,NA,NA,0,NA,"biological engineering; reproducibility; physical examination; medicine",2011,"Journal","Transactions of the Japan Society of Mechanical Engineers. C","Japan Society of Mechanical Engineers","10.1299/kikaic.77.138","Orthopaedic physical examination assisting system for improvement of accuracy and reproducibility of knee laxity diagnosis",0,NA
"Reproducibility",2771780683,"2168577915; 2148299636","Maddalena De Bernardo; Nicola Rosa",2,0,2,1,1,1,NA,NA,NA,1,"surgery; medicine; reproducibility; radiology",2017,"Journal","Journal of Glaucoma",NA,"10.1097/IJG.0000000000000843","Reproducibility of Central Corneal Thickness Measurements in Healthy and Glaucomatous Eyes",0,NA
"Reproducibility",2780086612,"2133039009; 2152926006; 2121463927","Oscar H. Del Brutto; Hector H. Garcia; Javier A. Bustos",3,0,0,0,0,0,0,NA,NA,0,"immunology; pathology; neurocysticercosis; biology; reproducibility",2017,"Journal","American Journal of Tropical Medicine and Hygiene",NA,"10.4269/ajtmh.17-0724b","Reproducibility of Diagnostic Criteria for Ventricular Neurocysticercosis",0,NA
"Reproducibility",2110787577,"2645247999","Ben Carterette",1,0,0,0,0,NA,NA,NA,NA,0,"statistical significance; computer science; reproducibility; information retrieval; text retrieval conference; statistical hypothesis testing; data mining; biomedical sciences",2014,"Conference",NA,"ACM","10.1145/2600428.2602292","Statistical significance testing in information retrieval: theory and practice",1,"The past 20 years have seen a great improvement in the rigor of information retrieval experimentation, due primarily to two factors: high-quality, public, portable test collections such as those produced by TREC (the Text REtrieval Conference), and the increased practice of statistical hypothesis testing to determine whether measured improvements can be ascribed to something other than random chance. Together these create a very useful standard for reviewers, program committees, and journal editors; work in information retrieval (IR) increasingly cannot be published unless it has been evaluated using a well-constructed test collection and shown to produce a statistically significant improvement over a good baseline. But, as the saying goes, any tool sharp enough to be useful is also sharp enough to be dangerous. Statistical tests of significance are widely misunderstood. Most researchers and developers treat them as a ""black box"": evaluation results go in and a p-value comes out. But because significance is such an important factor in determining what research directions to explore and what is published, using p-values obtained without thought can have consequences for everyone doing research in IR. Ioannidis has argued that the main consequence in the biomedical sciences is that most published research findings are false; could that be the case in IR as well?"
"Reproducibility",2318463192,"2679623471; 2591534458; 2117165463; 2160173928; 2125455454; 2091179548; 2021010695; 2116380018","Jun Okamoto; Yu Kumasaka; Kazuya Kawamura; Tomoyuki Matsumoto; Seiji Kubo; Hirotsugu Muratsu; Masahiro Kurosaka; Masakatsu G. Fujie",8,5,0,0,0,0,NA,NA,0,NA,"reproducibility; computer science; sampling error; physical examination; biological engineering",2011,"Journal","Transactions of the Japan Society of Mechanical Engineers. C","Japan Society of Mechanical Engineers","10.1299/kikaic.77.3780","Study of anatomical landmark sampling error effect on motion measurement reproducibility for Orthopaedic physical examination assisting system",0,NA
"Reproducibility",2113375489,"2092620728; 2739804707","M. T. Fernández Pareja; A. García Pablos",2,2,0,NA,NA,NA,NA,NA,NA,NA,"repeatability; engineering; computer vision; artificial intelligence; reproducibility; simulation; calibration; laser scanning",2013,"Conference",NA,"EDP Sciences","10.1051/metrology/201306001","Terrestrial Laser Scanner (TLS) equipment calibration",1,"This study's aim is to develop the measurement procedures to evaluate the quality of the observations made using TLS equipment. In addition, it seeks to estimate an uncertainty value allowing the users to know the reliability of the instrument measurements. The intention is to describe a working reproducible methodology without the need for a very complex infrastructure and based on ISO criteria. To achieve this, several TLS equipment based on different measurement principles were used. This study provides a good approach to the repeatability and reproducibility of the measurements to determine how the different measurements of the same measurand match."
"Reproducibility",2091644483,"2281638738; 2434205300","Xingdong Shen; Gang Dong",2,2,0,NA,NA,NA,NA,NA,NA,NA,"control engineering; ovality; clamp; clamping; cylinder; computer science; reproducibility; transfer function",2010,"Conference",NA,"International Society for Optics and Photonics","10.1117/12.887825","The calculation of the reproducibility error in the technological system with the clamp on the gas power",0,"The paper deduced a calculation formula by the classic control theory on the Reproducibility Error of parts
processing in the technology system, the flexible clamp system , with the clamping device on the gas power, and
analyzed its influencing factor with examples, the Reproducibility Error law: The larger the diameter of the
cylinder, the smaller the error; the slower the tool speed, (k values smaller), the smaller the error."
"Reproducibility",2482392649,"2131821156; 2157432659; 2170398916; 2170215785","Ruth G. Shaw; Allen J. Moore; Mohamed A. F. Noor; Michael G. Ritchie",4,0,1,0.25,1,0,0,0,NA,0,"credibility; documentation; reproducibility; vitality; biology; generality; management science; transparency",2016,"Journal","Ecology and Evolution","John Wiley and Sons Ltd","10.1002/ece3.2291","Transparency and reproducibility in evolutionary research",0,"The vitality of the science of evolutionary biology depends on new research findings that continually advance understanding, and the sound approaches by which those findings are obtained must be clearly documented. This transparency of documentation crucially grounds others' assessment of the credibility of the inferences and interpretations. It is also essential to attempts of future scientists, in conducting independent research, to validate the findings and assess their generality, advancing the field still further. These core aspects of science are widely appreciated. Even so, there is increasing awareness that publication practice sometimes falls well short of the goal of transparency that supports a thorough evaluation of the results, sufficient information for research synthesis, and the possibility of replicating of a study."
